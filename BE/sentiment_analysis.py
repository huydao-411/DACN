"""
Financial News Sentiment Analysis using FinBERT
================================================
Script phÃ¢n tÃ­ch Sentiment cho tin tá»©c tÃ i chÃ­nh tá»« CafeF
sá»­ dá»¥ng pre-trained model FinBERT tá»« Hugging Face

Features:
- Sá»­ dá»¥ng ProsusAI/finbert (chuyÃªn tÃ i chÃ­nh)
- Batch processing Ä‘á»ƒ tá»‘i Æ°u hiá»‡u nÄƒng
- Cache model trÃ¡nh load láº¡i
- Xá»­ lÃ½ text dÃ i
- TÃ­nh sentiment score trung bÃ¬nh
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from typing import Tuple, Dict, List
import warnings
warnings.filterwarnings('ignore')


class FinBERTSentimentAnalyzer:
    """
    Class phÃ¢n tÃ­ch sentiment sá»­ dá»¥ng FinBERT
    """
    
    # Class variable Ä‘á»ƒ cache model (trÃ¡nh load láº¡i)
    _model_cache = {}
    _tokenizer_cache = {}
    
    def __init__(self, model_name: str = "ProsusAI/finbert", device: str = None):
        """
        Khá»Ÿi táº¡o Sentiment Analyzer
        
        Args:
            model_name (str): TÃªn model tá»« Hugging Face
            device (str): 'cuda' hoáº·c 'cpu' (auto-detect náº¿u None)
        """
        self.model_name = model_name
        
        # Tá»± Ä‘á»™ng detect device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        print(f"[INFO] Device: {self.device}")
        print(f"[INFO] Model: {model_name}")
        
        # Load model tá»« cache hoáº·c download
        self._load_model()
        
        # Label mapping
        self.label2sentiment = {
            0: 'positive',
            1: 'negative',
            2: 'neutral'
        }
        
        self.sentiment2score = {
            'positive': 1,
            'negative': -1,
            'neutral': 0
        }
    
    def _load_model(self):
        """
        Load model vÃ  tokenizer tá»« cache hoáº·c download
        """
        try:
            print(f"[INFO] Äang load model '{self.model_name}'...")
            
            # Kiá»ƒm tra cache
            if self.model_name not in self._model_cache:
                print("[INFO] Model khÃ´ng trong cache, Ä‘ang download...")
                
                # Load tokenizer
                tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self._tokenizer_cache[self.model_name] = tokenizer
                
                # Load model
                model = AutoModelForSequenceClassification.from_pretrained(
                    self.model_name,
                    num_labels=3
                )
                model = model.to(self.device)
                model.eval()
                self._model_cache[self.model_name] = model
                
                print("[SUCCESS] Model downloaded vÃ  cached")
            else:
                print("[INFO] Model táº£i tá»« cache")
            
            self.model = self._model_cache[self.model_name]
            self.tokenizer = self._tokenizer_cache[self.model_name]
            
        except Exception as e:
            print(f"[ERROR] Lá»—i khi load model: {str(e)}")
            raise
    
    def _preprocess_text(self, text: str, max_length: int = 512) -> str:
        """
        Tiá»n xá»­ lÃ½ text
        
        Args:
            text (str): Text gá»‘c
            max_length (int): Äá»™ dÃ i tá»‘i Ä‘a
        
        Returns:
            str: Text Ä‘Ã£ xá»­ lÃ½
        """
        if not text or not isinstance(text, str):
            return ""
        
        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        text = text.strip()
        
        # Náº¿u text quÃ¡ dÃ i, cáº¯t bá»›t
        if len(text) > max_length:
            print(f"[WARNING] Text quÃ¡ dÃ i ({len(text)} chars), cáº¯t bá»›t Ä‘áº¿n {max_length}")
            text = text[:max_length]
        
        return text
    
    def analyze_sentiment(self, text: str) -> Tuple[str, float]:
        """
        PhÃ¢n tÃ­ch sentiment cá»§a má»™t text
        
        Args:
            text (str): Text cáº§n phÃ¢n tÃ­ch
        
        Returns:
            Tuple[str, float]: (label, confidence_score)
        """
        try:
            # Tiá»n xá»­ lÃ½
            text = self._preprocess_text(text)
            
            if not text:
                print("[WARNING] Text trá»‘ng, tráº£ vá» neutral")
                return 'neutral', 0.0
            
            # Tokenize
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Inference
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
            
            # Láº¥y probabilities
            probabilities = torch.softmax(logits, dim=-1)
            confidence, predicted_class = torch.max(probabilities, 1)
            
            # Convert to Python types
            label = self.label2sentiment[predicted_class.item()]
            confidence = confidence.item()
            
            return label, confidence
        
        except Exception as e:
            print(f"[ERROR] Lá»—i khi analyze: {str(e)}")
            return 'neutral', 0.0
    
    def analyze_batch(self, texts: List[str], batch_size: int = 8) -> List[Tuple[str, float]]:
        """
        PhÃ¢n tÃ­ch sentiment cho batch texts
        
        Args:
            texts (List[str]): Danh sÃ¡ch texts
            batch_size (int): KÃ­ch thÆ°á»›c batch
        
        Returns:
            List[Tuple[str, float]]: Danh sÃ¡ch (label, confidence)
        """
        results = []
        total = len(texts)
        
        print(f"\n[INFO] Äang phÃ¢n tÃ­ch {total} texts trong batch...")
        
        for i in range(0, total, batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_results = []
            
            try:
                # Tokenize batch
                inputs = self.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512,
                    padding=True
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # Inference
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits = outputs.logits
                
                # Process results
                probabilities = torch.softmax(logits, dim=-1)
                confidences, predicted_classes = torch.max(probabilities, 1)
                
                for j in range(len(batch_texts)):
                    label = self.label2sentiment[predicted_classes[j].item()]
                    confidence = confidences[j].item()
                    batch_results.append((label, confidence))
                
                results.extend(batch_results)
                
                # Progress
                progress = min(i + batch_size, total)
                print(f"[PROGRESS] {progress}/{total} texts processed...")
                
            except Exception as e:
                print(f"[ERROR] Lá»—i batch {i//batch_size}: {str(e)}")
                # Fallback: process individually
                for text in batch_texts:
                    label, conf = self.analyze_sentiment(text)
                    batch_results.append((label, conf))
                results.extend(batch_results)
        
        print(f"[SUCCESS] PhÃ¢n tÃ­ch hoÃ n thÃ nh {len(results)} texts\n")
        return results
    
    def calculate_sentiment_score(self, label: str, confidence: float) -> float:
        """
        TÃ­nh sentiment score tá»« label vÃ  confidence
        
        Args:
            label (str): Sentiment label
            confidence (float): Confidence score
        
        Returns:
            float: Sentiment score (-1 to 1)
        """
        base_score = self.sentiment2score.get(label, 0)
        weighted_score = base_score * confidence
        return weighted_score


def load_news_data(csv_file: str = "cafef_news.csv") -> pd.DataFrame:
    """
    Táº£i dá»¯ liá»‡u tin tá»©c tá»« CSV
    
    Args:
        csv_file (str): TÃªn file CSV
    
    Returns:
        pd.DataFrame: DataFrame chá»©a tin tá»©c
    """
    try:
        print(f"[INFO] Äang táº£i {csv_file}...")
        df = pd.read_csv(csv_file)
        print(f"[SUCCESS] Táº£i {len(df)} bÃ i viáº¿t")
        return df
    except Exception as e:
        print(f"[ERROR] Lá»—i táº£i file: {str(e)}")
        return pd.DataFrame()


def analyze_news_sentiment(df: pd.DataFrame, analyzer: FinBERTSentimentAnalyzer) -> pd.DataFrame:
    """
    PhÃ¢n tÃ­ch sentiment cho toÃ n bá»™ tin tá»©c
    
    Args:
        df (pd.DataFrame): DataFrame tin tá»©c
        analyzer (FinBERTSentimentAnalyzer): Analyzer object
    
    Returns:
        pd.DataFrame: DataFrame vá»›i sentiment columns
    """
    try:
        print("\n" + "="*80)
        print("PHÃ‚N TÃCH SENTIMENT TIN Tá»¨C")
        print("="*80)
        
        # Chuáº©n bá»‹ dá»¯ liá»‡u
        # Sá»­ dá»¥ng title + summary Ä‘á»ƒ cÃ³ enough context
        texts = []
        for idx, row in df.iterrows():
            title = str(row.get('title', '')).strip()
            summary = str(row.get('summary', '')).strip()
            combined_text = f"{title}. {summary}"
            texts.append(combined_text)
        
        # PhÃ¢n tÃ­ch batch
        results = analyzer.analyze_batch(texts, batch_size=8)
        
        # ThÃªm vÃ o DataFrame
        sentiment_labels = [label for label, conf in results]
        sentiment_confidences = [conf for label, conf in results]
        sentiment_scores = [
            analyzer.calculate_sentiment_score(label, conf)
            for label, conf in results
        ]
        
        df['sentiment_label'] = sentiment_labels
        df['sentiment_confidence'] = sentiment_confidences
        df['sentiment_score'] = sentiment_scores
        
        print("\n[SUCCESS] PhÃ¢n tÃ­ch sentiment hoÃ n thÃ nh")
        return df
    
    except Exception as e:
        print(f"[ERROR] Lá»—i khi phÃ¢n tÃ­ch: {str(e)}")
        return df


def display_sentiment_stats(df: pd.DataFrame):
    """
    Hiá»ƒn thá»‹ thá»‘ng kÃª sentiment
    
    Args:
        df (pd.DataFrame): DataFrame vá»›i sentiment columns
    """
    try:
        print("\n" + "="*80)
        print("THá»NG KÃŠ SENTIMENT")
        print("="*80)
        
        # PhÃ¢n bá»‘ sentiment
        print("\n[PhÃ¢n bá»‘ Sentiment Labels]")
        sentiment_dist = df['sentiment_label'].value_counts()
        for label, count in sentiment_dist.items():
            percentage = (count / len(df)) * 100
            print(f"  {label:10}: {count:3} ({percentage:5.1f}%)")
        
        # Thá»‘ng kÃª score
        print("\n[Thá»‘ng kÃª Sentiment Score]")
        print(f"  Mean:     {df['sentiment_score'].mean():7.4f}")
        print(f"  Std:      {df['sentiment_score'].std():7.4f}")
        print(f"  Min:      {df['sentiment_score'].min():7.4f}")
        print(f"  Max:      {df['sentiment_score'].max():7.4f}")
        print(f"  Median:   {df['sentiment_score'].median():7.4f}")
        
        # Confidence
        print("\n[Thá»‘ng kÃª Confidence]")
        print(f"  Mean:     {df['sentiment_confidence'].mean():7.4f}")
        print(f"  Min:      {df['sentiment_confidence'].min():7.4f}")
        print(f"  Max:      {df['sentiment_confidence'].max():7.4f}")
        
        # Overall Sentiment Score
        weighted_score = df['sentiment_score'].mean()
        print(f"\n[Overall Sentiment Score]: {weighted_score:7.4f}")
        if weighted_score > 0.1:
            print("  âžœ Trend: POSITIVE ðŸ“ˆ")
        elif weighted_score < -0.1:
            print("  âžœ Trend: NEGATIVE ðŸ“‰")
        else:
            print("  âžœ Trend: NEUTRAL âž¡ï¸")
        
    except Exception as e:
        print(f"[ERROR] Lá»—i khi hiá»ƒn thá»‹ stats: {str(e)}")


def display_sample_results(df: pd.DataFrame, num_samples: int = 5):
    """
    Hiá»ƒn thá»‹ máº«u káº¿t quáº£ phÃ¢n tÃ­ch
    
    Args:
        df (pd.DataFrame): DataFrame
        num_samples (int): Sá»‘ máº«u
    """
    try:
        print("\n" + "="*80)
        print(f"MáºªU {num_samples} BÃ€I VIáº¾T Vá»šI SENTIMENT")
        print("="*80)
        
        for idx, (i, row) in enumerate(df.head(num_samples).iterrows(), 1):
            title = str(row.get('title', 'N/A'))[:60]
            label = row.get('sentiment_label', 'N/A')
            conf = row.get('sentiment_confidence', 0)
            score = row.get('sentiment_score', 0)
            
            emoji = {'positive': 'ðŸ˜Š', 'negative': 'ðŸ˜ž', 'neutral': 'ðŸ˜'}.get(label, 'â“')
            
            print(f"\n[{idx}] {title}...")
            print(f"    Label: {label:10} {emoji}")
            print(f"    Confidence: {conf:.4f}")
            print(f"    Score: {score:7.4f}")
        
        print("\n" + "="*80)
    
    except Exception as e:
        print(f"[ERROR] Lá»—i khi hiá»ƒn thá»‹ máº«u: {str(e)}")


def plot_sentiment_distribution(df: pd.DataFrame, save_path: str = "sentiment_distribution.png"):
    """
    Váº½ biá»ƒu Ä‘á»“ phÃ¢n bá»‘ sentiment
    
    Args:
        df (pd.DataFrame): DataFrame
        save_path (str): ÄÆ°á»ng dáº«n lÆ°u file
    """
    try:
        print(f"\n[INFO] Váº½ biá»ƒu Ä‘á»“ sentiment distribution...")
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('Sentiment Analysis Results', fontsize=16, fontweight='bold')
        
        # 1. Sentiment Distribution (Pie Chart)
        ax1 = axes[0, 0]
        sentiment_counts = df['sentiment_label'].value_counts()
        colors = {'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#95a5a6'}
        colors_list = [colors.get(label, '#95a5a6') for label in sentiment_counts.index]
        
        ax1.pie(
            sentiment_counts.values,
            labels=sentiment_counts.index,
            autopct='%1.1f%%',
            colors=colors_list,
            startangle=90,
            textprops={'fontsize': 10}
        )
        ax1.set_title('Sentiment Distribution', fontweight='bold')
        
        # 2. Sentiment Score Distribution (Histogram)
        ax2 = axes[0, 1]
        ax2.hist(df['sentiment_score'], bins=20, color='#3498db', edgecolor='black', alpha=0.7)
        ax2.axvline(df['sentiment_score'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')
        ax2.set_xlabel('Sentiment Score', fontweight='bold')
        ax2.set_ylabel('Frequency', fontweight='bold')
        ax2.set_title('Sentiment Score Distribution', fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Confidence Distribution by Sentiment
        ax3 = axes[1, 0]
        for label in df['sentiment_label'].unique():
            data = df[df['sentiment_label'] == label]['sentiment_confidence']
            ax3.hist(data, label=label, alpha=0.6, bins=15)
        ax3.set_xlabel('Confidence Score', fontweight='bold')
        ax3.set_ylabel('Frequency', fontweight='bold')
        ax3.set_title('Confidence Distribution by Sentiment', fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. Score vs Confidence Scatter
        ax4 = axes[1, 1]
        scatter_colors = df['sentiment_label'].map(colors)
        ax4.scatter(
            df['sentiment_confidence'],
            df['sentiment_score'],
            c=scatter_colors,
            s=100,
            alpha=0.6,
            edgecolors='black'
        )
        ax4.set_xlabel('Confidence Score', fontweight='bold')
        ax4.set_ylabel('Sentiment Score', fontweight='bold')
        ax4.set_title('Score vs Confidence', fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        # Add legend
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='#2ecc71', label='Positive'),
            Patch(facecolor='#e74c3c', label='Negative'),
            Patch(facecolor='#95a5a6', label='Neutral')
        ]
        ax4.legend(handles=legend_elements, loc='upper left')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=100, bbox_inches='tight')
        print(f"[SUCCESS] Biá»ƒu Ä‘á»“ lÆ°u: {save_path}")
        plt.show()
    
    except Exception as e:
        print(f"[ERROR] Lá»—i váº½ biá»ƒu Ä‘á»“: {str(e)}")


def save_results(df: pd.DataFrame, output_file: str = "cafef_news_with_sentiment.csv"):
    """
    LÆ°u káº¿t quáº£ vÃ o CSV
    
    Args:
        df (pd.DataFrame): DataFrame
        output_file (str): TÃªn file output
    """
    try:
        print(f"\n[INFO] LÆ°u káº¿t quáº£ vÃ o {output_file}...")
        
        # Chá»n columns
        columns_to_save = [
            'title', 'url', 'publish_date', 'summary',
            'sentiment_label', 'sentiment_confidence', 'sentiment_score'
        ]
        
        # Lá»c cÃ¡c columns tá»“n táº¡i
        columns_to_save = [col for col in columns_to_save if col in df.columns]
        
        df_save = df[columns_to_save].copy()
        df_save.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print(f"[SUCCESS] LÆ°u {len(df_save)} bÃ i viáº¿t vÃ o {output_file}")
        return True
    
    except Exception as e:
        print(f"[ERROR] Lá»—i khi lÆ°u: {str(e)}")
        return False


def main():
    """
    HÃ m main - Ä‘iá»ƒm vÃ o chÃ­nh
    """
    print("\n" + "="*80)
    print("SENTIMENT ANALYSIS FOR FINANCIAL NEWS USING FINBERT")
    print("="*80)
    
    try:
        # 1. Load news data
        df = load_news_data("csv/cafef_news.csv")
        if df.empty:
            print("[ERROR] KhÃ´ng thá»ƒ táº£i dá»¯ liá»‡u tin tá»©c")
            return
        
        # 2. Khá»Ÿi táº¡o analyzer
        analyzer = FinBERTSentimentAnalyzer(model_name="ProsusAI/finbert")
        
        # 3. PhÃ¢n tÃ­ch sentiment
        df = analyze_news_sentiment(df, analyzer)
        
        # 4. Hiá»ƒn thá»‹ thá»‘ng kÃª
        display_sentiment_stats(df)
        
        # 5. Hiá»ƒn thá»‹ máº«u
        display_sample_results(df, num_samples=5)
        
        # 6. Váº½ biá»ƒu Ä‘á»“
        plot_sentiment_distribution(df, save_path="sentiment_distribution.png")
        
        # 7. LÆ°u káº¿t quáº£
        save_results(df, output_file="cafef_news_with_sentiment.csv")
        
        print("\n" + "="*80)
        print("[SUCCESS] PHÃ‚N TÃCH HOÃ€N THÃ€NH!")
        print("="*80)
        print("Output files:")
        print("  - cafef_news_with_sentiment.csv (Data with sentiment)")
        print("  - sentiment_distribution.png (Visualization)")
        print("="*80 + "\n")
        
    except Exception as e:
        print(f"\n[CRITICAL ERROR] {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
